---
title: "Are the more flexible great-tailed grackles also better at behavioral inhibition?"
author: 
 - '[Logan CJ](http://CorinaLogan.com)^1^'
 - '[McCune KB](https://www.kelseymccune.com)^2^'
 - '[MacPherson M](http://maggiepmacpherson.com)^2^'
 - 'Johnson-Ulrich Z^2^'
 - 'Bergeron L^2^'
 - 'Rowney C^1^'
 - '[Seitz B](https://benjaminseitz.wixsite.com/mysite)^3^'
 - '[Blaisdell A](http://pigeonrat.psych.ucla.edu)^3^'
 - 'Folsom M^1^'
 - 'Deffner D^1^'
 - '[Wascher C](https://www.claudiawascher.com)^4^'
date: '`r Sys.Date()`'
output:
  html_document: 
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
    code_folding: hide 
  md_document: 
    toc: true
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
  github_document: 
    toc: true
bibliography: MyLibrary.bib
csl: https://raw.githubusercontent.com/corinalogan/grackles/master/Files/behavioral-ecology.csl
header-includes:
  - \usepackage[left]{lineno}
  - \linenumbers
---

##### Affiliations: 
1) Max Planck Institute for Evolutionary Anthropology
2) University of California Santa Barbara
3) University of California Los Angeles
4) Anglia Ruskin University

*Corresponding author: corina_logan@eva.mpg.de

```{r setup, include=FALSE}
library(knitr)
#Make code wrap text so it doesn't go off the page when Knitting to PDF
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

knitr::opts_chunk$set(echo = TRUE)
```

<img width="50%" src="logoPCIecology.png">

**Cite as:** Logan CJ, McCune K, MacPherson M, Johnson-Ulrich Z, Bergeron L, Rowney C, Seitz B, Blaisdell A, Folsom M, Wascher CAF. 2019. **[Are the more flexible individuals also better at inhibition?](http://corinalogan.com/Preregistrations/g_inhibition.html)** (http://corinalogan.com/Preregistrations/g_inhibition.html) In principle acceptance by *PCI Ecology* of the version on 6 Mar 2019 [https://github.com/corinalogan/grackles/blob/master/Files/Preregistrations/g_inhibition.Rmd](https://github.com/corinalogan/grackles/blob/master/Files/Preregistrations/g_inhibition.Rmd).

<img width="5%" src="logoOpenAccess.png"> <img width="5%" src="logoOpenCode.png"> <img width="5%" src="logoOpenPeerReview.png">

**This preregistration has been pre-study peer reviewed and received an In Principle Recommendation by:**

Erin Vogel (2019) Adapting to a changing environment: advancing our understanding of the mechanisms that lead to behavioral flexibility. *Peer Community in Ecology*, 100016. [10.24072/pci.ecology.100016](https://ecology.peercommunityin.org/public/rec?id=32&reviews=True)

 - Reviewers: Simon Gingins and two anonymous reviewers

### ABSTRACT

It is thought that those individuals who are more behaviorally flexible (measured as reversal learning) must also be better at inhibiting because one should need to inhibit a previously learned behavior to change their behavior when the task changes [the flexibility component; @manrique_repeated_2013; @griffin2014innovation; @liu2016learning]. However, @homberg2007serotonin showed that rats with improved behavioral inhibition (due to gene knockouts) did not perform better in a reversal learning experiment than non-knockout rats. These results challenge the assumption that behavioral inhibition is involved in making flexible decisions. We test the hypothesis that behavioral flexibility is associated with behavioral inhibition. Touchscreen experiments had never been conducted in this species before, and it was one of our goals to validate whether this setup is viable for running a behavioral inhibition task on wild-caught adult grackles. We found that the grackles were able to learn to use the touchscreen and to complete the go/no go test on it, thus validating the use of this setup for future experiments. Results showed that only the go/no go performance positively correlated with the number of trials to reverse a preference, and only when using a go/no go passing criterion of number of trials to reach 85% correct, rather than needing to get 100% correct by trial 150 and 85% correct thereafter. Detour performance did not correlate with either measure of behavioral flexibility indicating that detour and reversal learning measure two separate traits. We were not able to run the delay of gratification experiment because the grackles never habituated to the apparatuses. Performance on the two behavioral inhibition tests, go/no go and detour, did not correlate with each other, indicating that they do not measure the same trait. Performance on the detour test was not affected by extensive experience obtaining food from tubes in the reversal learning test because grackles who received detour training before reversal training did not perform differently from those who received detour training after reversal training. We conclude that behavioral flexibility is associated with certain types of behavioral inhibition, but not others.

### [Video summary](https://youtu.be/TXFOYqZztf4)

### INTRODUCTION

It is thought that those individuals who are more behaviorally flexible (measured as reversal learning) must also be better at inhibiting because one should need to inhibit a previously learned behavior to change their behavior when the task changes [the flexibility component, see @mikhalevich_is_2017 for the theory behind the flexibility definition; @manrique_repeated_2013; @griffin2014innovation; @liu2016learning]. However, @homberg2007serotonin showed that rats with improved behavioral inhibition (due to gene knockouts) did not perform better in a reversal learning experiment than non-knockout rats. These results challenge the assumption that behavioral inhibition is involved in making flexible decisions.

Different forms of behavioral inhibition (active vs passive)...[Claudia]

We aimed to determine whether great-tailed grackles that are better at inhibiting behavioral responses in three experiments (delay of gratification, go-no go, detour) are also more behaviorally flexible (measured as reversal learning and solution switching on a puzzlebox by @logan2019flexmanip). Touchscreen experiments had never been conducted in this species before, and it was one of our goals to validate whether this set up is viable for running a behavioral inhibition task on wild-caught adult grackles. By measuring behavioral inhibition in three different ways, we aimed to determine whether these tests measure the same trait or separate traits. Results indicate whether flexibility and behavioral inhibition are related and whether behavioral inhibition tests measure the same trait.

### RESULTS

#### Prediction 1: go/no go

**Model 2a: Number of trials**

There was no correlation between the number of trials to pass criterion in the go/no go experiment and the number of trials to reverse a preference in the colored tube reversal experiment (in their **last reversal**) when using our original go/no go passing criterion (if 100% correct is not reached by trial 150, then they pass when they reach 85% correct after trial 150, measured in the most recent 20 trial block; Table 2). Nine grackles participated in the go/no go experiment and they passed criterion in an average of 178 trials (standard deviation: 15, range: 160 trials to not passing before the experiment ended at 200 trials).

##### Unregistered analyses

We additionally analyzed the relationship between go/no go performance and the number of trials to reverse a color preference in the **first reversal** to make our results comparable across more species. This is because most studies do not conduct serial reversals, but only one reversal. The results remained the same regardless of whether the first or last reversal were analyzed: there was no relationship between go/no go and reversal learning performance (Table 2).

As we conducted this experiment, we realized that setting an **arbitrary threshold** of needing 100% correct in the first 150 trials to pass criterion was not ecologically relevant for grackles. In reversal learning tests, which are similar to the go/no go experimental design in that they learn to discriminate between two shapes, grackles almost always continue to explore their options regardless of whether they already have a color preference [e.g., @logan2016flexibilityproblem]. Therefore, we *posthoc* examined the number of trials required to reach 85% correct in the most recent 20 trial block, regardless of the 150 trial threshold. Using this criterion, grackles passed in an average of 149 trials (standard deviation: 71, range: 60-290 trials). We found that there was more individual variation using this variable. Results showed that the number of trials to reach 85% correct positively correlated with the number of trials to reverse a preference. This indicates that those individuals that are faster at inhibiting are also faster at changing their preferences when circumstances change. However, we are hesitant to rely on the result because 1) it was a posthoc analysis, and 2) the relationship appeared to occur because of the outlier (Taquito). We would need a larger sample size to determine whether this is a reliable result. 

**Table 1.** Summarized results per bird in the go/no go and detour behavioral inhibition experiments, and the reversal and multi-access box (MAB) flexibility experiments. **Detour proportion correct modified** accounts for the grackle-specific behavior of standing at the opening of the tube where they are about to reach their head inside the tube to get the food, but they appear frustrated and bite the edge of the plastic tube. These bites do not count as first touch to the plastic when the bird obtains the food immediately after the bite.

```{r summary, eval=TRUE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)

d <- data.frame(d)
colnames(d) <- c("Bird","Go/no go trials to 85% correct after 150 trials","Go/no go trials to 85% correct","Detour proportion correct","Detour proportion correct modified","Detour pre- or post-reversal","Trials to reverse in first reversal","Trials to reverse in last reversal","Average latency to attempt new solution (MAB)")

library(kableExtra)
knitr::kable(d) %>%
kable_styling(full_width = T, position = "left")
```

**Table 2.** Results from the GLMs: **m1** and **m2** show GLM outputs for the last reversal, while **m3** and **m4** show GLM outputs for the first reversal. **m1** and **m3** show results from the GLM using the number of trials to reach 85% correct if 100% correct was not achieved within the first 150 trials in go/no go, while **m2** and **m4** use the number of trials to reach 85% correct without the 150 trial threshold.

```{r goResults, eval=T, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","Gonogo150","Gonogo85","Detour","DetourA","Detourprepost","TrialsFirst","TrialsLast","AvgLatency")

#GLM
m1 <- glm(Gonogo150 ~ TrialsLast, family="poisson", data=d)
#sm1 <- summary(m1) 
#there is no relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the last reversal)

m2 <- glm(Gonogo85 ~ TrialsLast, family="poisson", data=d)
#sm2 <- summary(m2) 
#there is a relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the last reversal) 

#have a look at the plot to see why m2, but not m1 is positively related
#plot(Gonogo85~TrialsLast, data=d) 
#Taquito is a huge outlier here and if I look at the rest of the data, I don't see a positive relationship. Need more data to know whether this relationship holds


#UNREGISTERED analyses: number of trials to reverse in the first reversal (which will make this data comparable with more species)
u1 <- glm(Gonogo150 ~ TrialsFirst, family="poisson", data=d)
#su1 <- summary(u1) 
#there is no relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the first reversal)

u2 <- glm(Gonogo85 ~ TrialsFirst, family="poisson", data=d)
#su2 <- summary(u2) 
#there is a relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the first reversal) (though the estimate 0.008 and the standard error is lower than the estimate so it doesn't cross zero, it is significant: p<2e-16)


#load packages for the output table
library(jtools) 
base::suppressMessages(jtools::export_summs(m1, m2, u1, u2, model.names=c("m1: 150 last reversal","m2: 85 last reversal","m3: 150 first reversal","m4: 85 first reversal"), digits = getOption("jtools-digits", default = 2), model.info = getOption("summ-model.info", TRUE), model.fit = getOption("summ-model.fit", TRUE), pvals = getOption("summ-pvals", FALSE))) 
#suppressMessages gets rid of the text saying that the broom package overwrites something in jtools. Need to specify the package before the function to avoid a message popping up and preventing the PDF from rendering
knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)}) 
#this prevents an issue with xcolor package when rendering to PDF

#Note: this code also works for showing output tables, but don't know how to get it to show more than one model at a time
#library(xtable)
#sm1.table <- xtable(sm1)
#library(knitr)
#kable(sm1.table, caption="Table T: Model selection output.", format="html", digits=2)
```

**Model 2b: Latency to peck screen**

The model examining the latency of the first peck to the screen per trial (response variable) in association with whether the response was correct or not and the trial number did not converge. This is probably due to the fact that the correct choice on the no go trials was not to peck the screen and so this level of the categorical choice variable has much less data than the other two levels (incorrect choice and correct choice on the go trials; Figure 1). Therefore, we cannot include the analysis here or make conclusions based on it. Therefore, we cannot include the analysis here or make conclusions based on it. Additionally, there was a problem matching the latency data across data sheets. Latency data was brought in from the PsychoPy data sheets, however the number of trials reported by the experimenter and by PsychoPy sometimes differed for reasons that are unclear. Therefore, the first latency to peck the screen is not completely accurately matched between the two data sheets.

```{r golatencyResults, eval=T, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_data_gonogoPsychopy.csv"), header=T, sep=",", stringsAsFactors=F)
d <- data.frame(d)

#load libraries
library(dplyr)
library(ggplot2)
library(cowplot)

#Remove Queso (bird 31) who didn't complete this experiment
d1 <- d[!d$bird==31,] 

#Set numeric and factors so MCMCglmm will run
d1$Latency <- as.numeric(d1$Latency)
d1$TrialThisSession <- as.numeric(d1$TrialThisSession)
d1$session <- as.factor(d1$session)
d1$bird <- as.factor(d1$bird)
d1$rewarded <- as.factor(d1$rewarded)

#Get rid of rows where rewarded=NA because we won't know whether it was a go or a no go trial
d1 <- subset(d1,!(is.na(d1["rewarded"])) & !(is.na(d1["Latency"])))
length(d1$Latency)


#FIGURE mean latency of first peck to screen ALL BIRDS by go or no go: x=trial number,  y=latency to first peck to screen/trial, by go (filled circles) or no go (open circles)

#for loops to scroll through the Latency column and only select the first latency to peck/trial
d2 <- matrix(data=NA,nrow=length(levels(d1$bird)), ncol=5) 
d2 <- data.frame(d2) 
colnames(d2) <- c("bird","session","trial","trialtype","firstlatency")
head(d2)
	
currentrow<-1
for(j in 1:length(levels(d1$bird))) {
  
  birdLatencies <- filter(d1,bird==levels(d1$bird)[j])
  
  for(i in 1:length(unique(birdLatencies$session))) {
    
    sessionLatencies <- filter(birdLatencies,session==unique(birdLatencies$session)[i],)
    
    for(h in 1:length(unique(sessionLatencies$TrialThisSession))) {
  
      trialLatencies <- filter(sessionLatencies,TrialThisSession==unique(sessionLatencies$TrialThisSession)[h])
      
        #Only take the first Latency per Trial
				d2[currentrow,]$firstlatency <- trialLatencies$Latency[1]
				d2[currentrow,]$trial <- trialLatencies$TrialThisSession[1]
				d2[currentrow,]$session <- as.character(trialLatencies$session[1])
				d2[currentrow,]$bird <- as.character(trialLatencies$bird[1])

				if (trialLatencies$rewarded[1]=="yes"){
				  d2[currentrow,]$trialtype <- "go"
				}
				
				else{
				  d2[currentrow,]$trialtype <- "no go"
				}
				
				currentrow<-currentrow+1
			}
		}
  }

## now get mean latency for each trial type for each bird for each session
#make factors (& then properly order session)
d2$bird <- as.factor(d2$bird)
d2$trialtype <- as.factor(d2$trialtype)
d2$session <- as.factor(d2$session)
levels(d2$session)
d2$session = factor(d2$session,levels(d2$session)[c(1,10:17,2:9)])
levels(d2$session)

#make new data frame for 2nd for loop data
d3 <- matrix(data=NA,nrow=length(levels(d1$bird)), ncol=4) 
d3 <- data.frame(d3) 
colnames(d3) <- c("bird","session","trialtype","avgfirstlatency")
head(d3)
	
currentrow<-1
for(a in 1:length(levels(d2$bird))) {
  
  birds <- filter(d2,bird==levels(d2$bird)[a])

  for(b in 1:length(unique(birds$session))) {
    
    sessions <- filter(birds,session==unique(birds$session)[b],)

      for(d in 1:length(unique(birds$trialtype))) {
  
			  d3[currentrow,]$avgfirstlatency <- mean(sessions[sessions$trialtype==unique(birds$trialtype)[d],]$firstlatency)
			  #unique says: run through trialtype levels and calculate mean first latency (the first factor level is go, so it runs through one first), then run through the data sheet again for the next factor level (no go) and because there are only 2 factor levels it then stops
			  
				d3[currentrow,]$session <- as.character(sessions$session[1])
				d3[currentrow,]$bird <- as.character(sessions$bird[1])
				d3[currentrow,]$trialtype <- as.character(unique(birds$trialtype)[d])
				currentrow<-currentrow+1
		}
  }
}

#Print the data to a sheet for Aaron
length(d3$trialtype) #218
write.csv(d3,file="g_inhibition_data_gonogoPsychopySummary.csv", sep=",",quote=FALSE,col.names=TRUE)  #It puts it in my grackle prereg github folder


#FIGURE: mean latency/session for go and no go trials against each other across ALL BIRDS
#make factors (& then properly order session)
d3$trialtype <- as.factor(d3$trialtype)
d3$session <- as.factor(d3$session)
levels(d3$session)
d3$session = factor(d3$session,levels(d3$session)[c(1,10:17,2:9)])
levels(d3$session) #17 sessions

#make new data frame for new for loop data
d4 <- matrix(data=NA,nrow=17*2, ncol=3) #17=number of rows
d4 <- data.frame(d4) 
colnames(d4) <- c("session","trialtype","avgfirstlatency")
head(d4)

currentrow<-1
for(e in 1:length(levels(d3$session))) {
    
    sessions1 <- filter(d3,session==levels(d3$session)[e])

      for(f in 1:length(unique(sessions1$trialtype))) {
  
			  d4[currentrow,]$avgfirstlatency <- mean(sessions1[sessions1$trialtype==unique(d3$trialtype)[f],]$avgfirstlatency,na.rm=T)
				d4[currentrow,]$session <- as.character(sessions1$session[1])
				d4[currentrow,]$trialtype <- as.character(unique(d3$trialtype)[f])
				currentrow<-currentrow+1
		}
  }

#Separate the data frame into go trials and no go trials
go <- d4[d4$trialtype=="go",]
no <- d4[d4$trialtype=="no go",]

op <- par(mfrow=c(1,1), oma=c(0,0,0,0), mar=c(4.5,4.5,2,0.2), cex.lab=1.8, cex.axis=2)
plot(go$session,go$avgfirstlatency, ylab="Average latency first peck (s)", xlab="Session", ylim=c(0,10), xlim=c(0,20), cex=2, pch=19)
abline(lm(go$avgfirstlatency~as.numeric(go$session)))
points(no$session,no$avgfirstlatency, ylab="Average latency first peck (s)", xlab="Session", ylim=c(0,10), xlim=c(0,20), cex=2, pch=1)
abline(lm(no$avgfirstlatency~as.numeric(no$session)),lty=2)
legend(x="topright", y=8, legend=c(pch19="go", pch1="no go"), pch=c(19,1), box.lty=1, cex=2)
par(op)
```

**Figure 1.** The latency to the first peck on the screen per trial across trials according to whether they made the incorrect choice by pecking during the no go trials or not pecking during the go trials (top), the correct choice on the Go trials by pecking the rewarded stimulus (middle), or the correct choice on the No Go trials by not pecking the unrewarded stimulus (bottom). For the latter, although these trials were coded as the bird making the correct choice by not pecking the screen, due to issues in combining the two data sheets (noted above), there were sometimes screen pecks listed for these trials.

#### Prediction 1: detour

There was no correlation between the number of trials to pass criterion in the go/no go experiment and the percent correct on the detour experiment (Table 3). 18 grackles completed this experiment and they averaged 71% correct (standard deviation: 25%, range: 20-100%). 

##### Unregistered analyses

We additionally analyzed the relationship between detour performance and the number of trials to reverse a color preference in the **first reversal** to make our results comparable across more species. This is because most studies do not conduct serial reversals, but only one reversal. The results remained the same regardless of whether the first or last reversal were analyzed: there was no relationship between detour and reversal learning performance (Table 3).

As we conducted this experiment, we discovered the scoring whether the grackle made a correct or incorrect first choice is more complicated than the scoring method used in @maclean2014evolution. In @maclean2014evolution, if the plastic is touched first, then it is an incorrect choice, whereas if the food is touched first, it is a correct choice. If the plastic is touched first, it is assumed that the individual touched the plastic on the long side of the tube and not on the rim side where the opening is because they were trying to reach the food through plastic (which is non-functional). We found that many grackles have a habit of standing at the tube opening attempting to reach their head in to obtain the food, but being hesitant to put their head inside the tube, at which point they bite the rim of the tube (what we have been calling a "frustration bite") just before reaching their head in to obtain the food. Because the grackle was clearly not attempting to reach the food through the plastic, we coded an additional variable, the "grackle-specific correct choice". In this variable, a frustration bite to the plastic rim does not count as an incorrect choice if they immediately afterward obtain the food. Instead, this counts as a correct choice. We therefore conducted *posthoc* analyses of the percent correct on the detour task in relation to their reversal performance (Table 3). The results were the same as above: there is no correlation between detour performance (using the grackle-specific correct choice) and the number of trials to reverse their last or first preference. With this scoring method, grackles averaged 87% correct (standard deviation: 25%, range: 60-100%).

**Table 3.** Results from the GLMs: **m1** and **m2** show GLM outputs using the  @maclean2014evolution method of scoring, while **m3** and **m4** show GLM outputs using the grackle-specific scoring method. **m1** and **m3** show results using the last reversal, while **m2** and **m4** use the first reversal.

```{r detourResults, eval=T, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","Gonogo150","Gonogo85","Detour","DetourA","Detourprepost","TrialsFirst","TrialsLast","AvgLatency")

#GLM
d$Bird <- factor(d$Bird)
m4 <- glm(Detour ~ TrialsLast, family="binomial", data=d)
sm4 <- summary(m4) 
#there is no relationship between the proportion of trials correct on the detour task and the number of trials to reverse in the last reversal

#UNREGISTERED ANALYSIS: first reversal (rather than last reversal)
m5 <- glm(Detour ~ TrialsFirst, family="binomial", data=d)
sm5 <- summary(m5) 
#there is no relationship between the proportion of trials correct on the detour task and the number of trials to reverse in the first reversal

m6 <- glm(DetourA ~ TrialsLast, family="binomial", data=d)
sm6 <- summary(m6) 
#there is no relationship between the proportion of trials correct on the detour task (grackle calculation) and the number of trials to reverse in the last reversal

m7 <- glm(Detour ~ TrialsFirst, family="binomial", data=d)
sm7 <- summary(m7) 
#there is no relationship between the proportion of trials correct on the detour task (grackle calculation) and the number of trials to reverse in the first reversal


#load packages for the output table
library(jtools) 
base::suppressMessages(jtools::export_summs(m4, m5, m6, m7, model.names=c("m1: MacLean calculation & last reversal","m2: MacLean calculation & first reversal","m3: Grackle calculation & last reversal","m4: Grackle calculation & first reversal"), digits = getOption("jtools-digits", default = 2), model.info = getOption("summ-model.info", TRUE), model.fit = getOption("summ-model.fit", TRUE), pvals = getOption("summ-pvals", FALSE))) 
#suppressMessages gets rid of the text saying that the broom package overwrites something in jtools. Need to specify the package before the function to avoid a message popping up and preventing the PDF from rendering
knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)}) 
#this prevents an issue with xcolor package when rendering to PDF
```

#### Prediction 2: correlation across behavioral inhibition tasks

There was no correlation between behavioral inhibition tasks, go/no go and detour, as indicated by the low reliability represented by Cronbach's alpha, which equalled zero for all comparisons (go/no go 150 threshold and detour standard=0.03, go/no go 150 and detour grackle specific=0.03, go/no go 85 and detour standard=0.005, go/no go 85 and detour grackle specific=0.003).

#### Prediction 3: does training improve detour performance?

There was no difference in the percent correct on the detour task and whether the individual received the detour experiment before or after their reversal learning experiment (which also involved obtaining food from tubes; Table 4).

##### Unregistered analysis

We conducted a post-hoc analysis using the grackle-specific percent of correct responses (see full explanation in P1: detour > Unregistered analyses) and found that the result is the same as above: there is no difference in detour performance relative to their experience with reversal tubes.

**Table 4.** Results from the GLMs: **Detour standard** shows GLM outputs using the  @maclean2014evolution method of scoring, and **Detour grackle-specific** shows GLM outputs using the grackle-specific scoring method.

```{r detour2Results, eval=T, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","Gonogo150","Gonogo85","Detour","DetourA","Condition","TrialsFirst","TrialsLast","AvgLatency")

#GLM - detour standard correct choice scoring
de <- glm(Detour ~ Condition, family="binomial", data=d)
sde <- summary(de)
#no difference between detour percent correct and whether they received detour before or after reversal tubes

#GLM - detour grackle-specific correct choice scoring - UNREGISTERED ANALYSIS
de2 <- glm(DetourA ~ Condition, family="binomial", data=d)
sde2 <- summary(de2)
#no difference between detour percent correct and whether they received detour before or after reversal tubes

#load packages for the output table
library(jtools) 
base::suppressMessages(jtools::export_summs(de, de2, model.names=c("Detour standard","Detour grackle-specific"), digits = getOption("jtools-digits", default = 2), model.info = getOption("summ-model.info", TRUE), model.fit = getOption("summ-model.fit", TRUE), pvals = getOption("summ-pvals", FALSE))) 
#suppressMessages gets rid of the text saying that the broom package overwrites something in jtools. Need to specify the package before the function to avoid a message popping up and preventing the PDF from rendering
knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)}) 
#this prevents an issue with xcolor package when rendering to PDF
```

NOTE: We were not able to conduct the delay of gratification experiment because the grackles never habituated to the apparatuses, therefore the behavioral inhibition results come from the go/no go and detour experiments. 


### DISCUSSION

We found mixed support for the hypothesis that behavioral flexibility and behavioral inhibition are associated with each other. These traits were positively associated in one context (go/no go) and not associated in another context (detour). This mixed support could be because the two behavioral inhibition tests, go/no go and detour, did not measure the same trait: there was no correlation of performances between the two experiments. This calls into question which behavioral inhibition experiment is a better measure of behavioral inhibition.

While conducting these tests for the first time with this species, we validated a variety of experimental details. We used **two different passing criteria** for the go/no go experiment and found that the criterion using the number of trials to reach 85% correct was the most useful and the one we recommend using in future research. The other criterion of needing to reach 100% correct in the first 150 trials and, if not achieved, 85% correct thereafter resulted in less individual variation. It is also not appropriate for this species, which continues to sample their environment even after having a preference as shown by their reversal learning performances: even after forming a color preference, most grackles continue to occasionally check the other option even though it does not contain a reward. This highlights the need to validate passing criteria before determining which are the best measures of the ability under investigation.

Although this species had never experienced **touchscreen experiments** before, we found that the grackles were able to learn to use the touchscreen and to complete the go/no go experiment on it. This validates the use of this setup for future experiments in this species, and shows that it could be a viable option for wild-caught adult birds from other species as well. However, the touchscreen came with its own difficulties. The automatically generated data sheets are not enough to score bird performance because the apparatus often malfunctions by not registering a bill peck (grackles often peck very softly), thus the experimenter had to sit in the aisle and be ready to remotely control the computer during every test session. Likely because of this, it was difficult to match the experimenter-generated data sheets, which were scored live and occasionally afterward by watching the videos, with the automatically-generated data sheets. Additionally, there was less than ideal interrater reliability when a second rater coded 20% of the videos to score whether the bird made the correct choice or not. We think this might be due to the fact that the correct choice data were not as objective to code as we had hoped due to the touchscreen malfunctioning (not registering touches to the screen), and to the subjective criterion that the bird had to be within a certain distance of the screen to be considered as paying attention to the screen and thus be in position to make a choice or not. This indicates that our touchscreen setup could be greatly improved such that it is actually automated, rather than needing experimenter intervention for every trial.

Performance on the detour test was not affected by **extensive experience obtaining food from tubes** in the reversal learning test. Grackles who received detour training before reversal training did not perform differently from those who received detour training after reversal training. These two contexts appear to be different enough to solicit independent responses without interference due to a grackleâ€™s previous test history. The development of our **grackle-relevant detour scoring method** resulted in improved performance for 9 out of the 16 grackles we tested. This indicates that cross-species comparisons on this test that are not attuned to the species under study could underestimate the potential performance the species is capable of. This could be one explanation for why so many of the 36 species in @maclean2014evolution performed so poorly on this task.

Our developments and modifications to these behavioral inhibition tests indicate that it is necessary to accommodate species-relevant behavioral differences in apparatus design and when making scoring choices to measure the actual potential of a given species. Such developments are required to determine what behavioral inhibition tests measure, whether it is appropriate to categorize different tests as measuring the same ability, and how behavioral inhibition relates to other traits.


### A. STATE OF THE DATA

NOTE: all parts of the preregistration are included in this one manuscript.

**Prior to collecting any data:** This preregistration was written. 

**After data collection had begun (and before any data analysis):** This preregistration was submitted to PCI Ecology (Oct 2018) for peer review after starting data collection on the detour task for the pre-reversal subcategory of subjects (for which there was data from one bird). Reviews were received, the preregistration was revised and resubmitted to PCI Ecology (Jan 2019) at which point there was detour data for six birds, data on a few training trials for the delay of gratification task for one bird, and no data from the go/no go experiment. This preregistration passed peer review and was recommended by PCI Ecology in March 2019 (see the [review history](https://ecology.peercommunityin.org/public/rec?id=32&reviews=True)).

**After data collection began and before data analysis: how the actual methods differed from the planned methods**

1) Jan 2020: we discovered that none of the grackles reached 100% accuracy within 150 trials (at least not at the level of 20 trial blocks), which is consistent with their reversal performance as well where they usually do not 100% prefer one option, but continue to occasionally explore the other option. The passing criterion of 100% correct within 150 trials or 85% correct between 150-200 trials could be the reason there was not much individual variation in this test (passing in 160-190 trials or they did not reach 85% accuracy within 200 trials). All grackles received 150+ trials, therefore we were only measuring variation after 150 trials, rather than variation across all trials. We decided to add a **post-hoc passing criterion** that might be more illustrative of individual differences in behavioral inhibition in grackles: 85% accuracy at the level of the most recent sliding 10 trial block (i.e., the most recent 10 trials, regardless of whether it is an even 20, 30, 40 trials). We added this modified response variable posthoc to the discussion. We predict this new passing criterion will show more individual variation, and that it will more accurately represent individual differences in grackle inhibitive abilities.

2) Jul 2020: Independent variables > P1 go/no go > Model 2b: removed the variable "flexibility condition" because, by definition, the birds in the manipulated condition were faster to reverse. 

3) Sep 2020: Prediction 1 alternative 2 analysis - when we tried to run the code we discovered that the Cronbach's alpha is not the appropriate test to run on our experimental design to test the internal validity of the experiment (e.g., does this test actually measure what we think it does). To test internal validity, we would need to change the experimental design, which was not the goal of our current study. Therefore, we did not conduct this analysis.

### B. PARTITIONING THE RESULTS

We may decide to present the results from different tests in separate papers. NOTE: everything in the preregistration is included in this one manuscript.

### C. HYPOTHESIS

**If flexibility requires behavioral inhibition, then individuals that are more [behaviorally flexible](./g_flexmanip.Rmd) (indicated by individuals that are faster at functionally changing their behavior when circumstances change), as measured by reversal learning and switching to a different option after one becomes non-functional on a multi-access box, will also be better at inhibiting their responses in three tasks: delayed gratification, go/no go, and detour.**

**P1:** Individuals that are faster to reverse preferences on a reversal learning task and who also have lower latencies to successfully solve new loci after previously solved loci become unavailable (multi-access box) (see [flexibility preregistration](./g_flexmanip.Rmd)) will perform better in the go/no go task (methods similar to @harding2004animal) and in the detour task (methods as in @maclean2014evolution who call it the "cylinder task"), and they will wait longer for higher quality (more preferred) food, but not for higher quantities of food (methods as in @hillemann2014waiting). Waiting for higher quality food has been validated as a test of behavioral inhibition in birds, while waiting for a higher quantity of food does not appear to measure behavioral inhibition (@hillemann2014waiting).

**P1 alternative 1:** If there is no correlation between flexibility measures and performance on the behavioral inhibition tasks, this may indicate that the flexibility tasks may not require much behavioral inhibition (particularly if the behavioral inhibition results are reliable - see *P1 alternative 2*).

**P1 alternative 2:** If there is no correlation between flexibility measures and performance on the behavioral inhibition tasks, this may indicate that the behavioral inhibition tasks had low reliability and were therefore too noisy to correlate with flexibility.

**P2:** If there is no correlation in performance across behavioral inhibition tasks, it may indicate that that one or more of these tasks does not measure behavioral inhibition, or that they measure different types of behavioral inhibition (see @friedman2004relations).

**P2 alternative:** If go/no go task performance strongly correlates with performance on the delayed gratification task, this indicates these two tasks measure the same trait, which therefore validates a behavioral inhibition task using a touchscreen (the go/no go task).

**P3:** If individuals perform well on the detour task and with little individual variation, this is potentially because they will have had extensive experience looking into the sides of opaque tubes during reversal learning. To determine whether prior experience with opaque tubes in reversal learning contributed to their detour performance, a subset of individuals will experience the detour task before any reversal learning tests. If this subset performs the same as the others, then previous experience with tubes does not influence detour task performance. If the subset performs worse than the others, this indicates that detour task performance depends on the previous experiences of the individuals tested.

![Figure 2. Experimental design.](g_inhibitionFig1.png)

***Figure 2.*** The experimental designs of the three tasks: delayed gratification, go/no go, and detour (see [protocol](https://docs.google.com/document/d/1oEQ66yLrkMFr4UJTXfPBRAEXqoUuOgRwcKOB_KcT7HE/edit?usp=sharing) for details). In the **delay of gratification** task, individuals learn that food items will be transferred by the experimenter from a storing lid (near the experimenter) to a serving lid (near the bird) one at a time, and that they have access to the food in the serving lid from which they can eat at any time: they will learn that they will have access to more food if they wait longer for the experimenter to transfer food items. Once they pass training (by waiting for more than one food item in three trials), they move on to the test where food items are transferred from the serving to the storing lid with delays ranging from 2-1280 seconds. Birds will be tested on whether they are willing to wait for food items that increase in quality (i.e., are more preferred) or increase in quantity (i.e., the same food type accumulates in the serving lid). In the **go/no go** task, after pecking a start key on the touchscreen to show they are interested in attending to a trial, they will see either a green circle or a purple circle (the rewarded circle color is counterbalanced across birds). Pecking the food key while the rewarded colored circle (green in the figure) is on the screen will result in the food hopper rising so the bird can eat food for 2 seconds, after which point the trial ends and the screen goes blank for 8 seconds before starting over again. If the non-rewarded colored circle (purple in the figure) appears on the screen after the start key is pecked, then the correct response is to refrain from pecking the food key for 10 seconds. If the bird succeeds in refraining, the next intertrial interval starts. If the bird fails and pecks the food key while the purple circle is on the screen, then it is given an aversive stimuli for 5 seconds (TV static screen). In the **detour** task, individuals first receive a warm up with an opaque tube where they learn that the experimenter will show them a piece of food and then move that piece of food into the tube. They then have the opportunity to approach the tube and eat the food. A correct response is when their first approach is to go to the side of the tube to the opening to obtain the food and an incorrect response is when they try to access the food by pecking at the front of the tube (which has no opening). Once they pass the warm up, they move on to the test, which is exactly the same except the tube is transparent. The idea is that being able to see the food through the tube wall might entice them to try to go through the wall rather than refrain from a direct approach to the food and instead go around the side through the tube opening.

### D. METHODS

#### Open materials

[Testing protocols](https://docs.google.com/document/d/1oEQ66yLrkMFr4UJTXfPBRAEXqoUuOgRwcKOB_KcT7HE/edit?usp=sharing) for all three experiments: color tube reversal learning, multi-access box, and touchscreen reversal learning

#### Open data

When the study is complete, the data will be published in the Knowledge Network for Biocomplexity's data repository.

#### Randomization and counterbalancing

**P3** 

Two individuals from each batch will experience the detour task before participating in the flexibility manipulation. These individuals will be randomly selected using the random number generator at https://www.random.org.

**P1-P2**

For the rest of the individuals (n=6 per batch), the order of the three behavioral inhibition tasks will be counterbalanced across birds (using https://www.random.org to randomly assign individuals to one of three experimental orders). 1/3 of the individuals will experience:

1. Delayed gratification task

2. Go/no go task

3. Detour

1/3 of the individuals will experience:

1. Go/no go task

2. Detour

3. Delayed gratification task

1/3 of the individuals will experience: 

1. Detour

2. Delayed gratification task

3. Go/no go task

**Delayed gratification** 

- Food preference test: food will be presented in random combinations over six sessions of 12-15 trials. 

- Training trials: The type of demonstration and training trials varied randomly (with more demo trials near the beginning of training), incorporating trials in which food of the same sort accumulated (quantity), food of ascending quality accumulated (quality), and trials in which we added increasingly larger food pieces throughout the trial (size).

- Test: we will test each food quality (low, mid, high) twice in randomized order in each session.

**Go/no go** 

Go and no go trials will be presented randomly with the restriction that no more than four of the same type will occur in a row. The rewarded color will be counterbalanced across birds.

**Detour** 

The side from which the apparatus is baited will be consistent within subjects, but counterbalanced across subjects.

#### Blinding of conditions during analysis

No blinding is involved in this study.

#### Dependent variables

##### *P1: the more flexible individuals are better at behavioral inhibition*

1) **Delayed gratification:** Number of food pieces waited for (0-3). A successful wait is defined as waiting for at least one additional piece of food to be added to the serving lid of the three possible additional food items, and accepting at least one of the reward pieces.

2) **Go/no go:** 

    a) The number of trials to reach criterion (85% correct) where correct responses involve pecking when the rewarded stimulus is displayed and not pecking when the unrewarded stimulus is displayed, and incorrect responses involve pecking when the unrewarded stimulus is displayed, and not pecking when the rewarded stimulus is displayed

    b) The latency to respond (peck the target key)

3) **Detour:** First approach (physical contact): Correct (to the tube's side opening) or Incorrect (to the front closed area of the tube) (methods as in @maclean2014evolution).

One model will be run per dependent variable.

##### *P3: does training improve detour performance?*

1) First approach (physical contact): Correct (to the tube's side opening) or Incorrect (to the front closed area of the tube) (methods as in @maclean2014evolution).

#### Independent variables

##### *P1: delayed gratification*

1) Food quality or quantity (Quality: High, Med, Low; Quantity: Smaller, Medium, Larger)

2) Trial

3) Delay (2, 5, 10, 20, 40, 60, or 80 seconds)

4) Flexibility 1: **Number of trials to reverse** a preference in the last reversal an individual experienced (reversal learning; an individual is considered to have a preference if it chose the rewarded option at least 17 out of the most recent 20 trials, with a minimum of 8 or 9 correct choices out of 10 on the two most recent sets of 10 trials). See behavioral flexibility [preregistration](./g_flexmanip.Rmd).

5) Flexibility 3: If the number of trials to reverse a preference does not positively correlate with the latency to attempt or solve new loci on the multi-access box (an additional measure of behavioral flexibility), then the **average latency to solve** and the **average latency to attempt** a new option on the multi-access box will be additional dependent variables. See behavioral flexibility [preregistration](./g_flexmanip.Rmd).

7) Flexibility 4: This measure is currently being developed and is intended to be a more accurate representation of all of the choices an individual made, as well as accounting for the degree of uncertainty exhibited by individuals as preferences change. If this measure more effectively represents flexibility (determined using a modeled dataset and not the actual data), we may decide to solely rely on this measure and not use flexibility measures 1 through 3. If this ends up being the case, we will modify the code in the analysis plan below to reflect this change.

##### *P1: go/no go*

Model 2a: number of trials to reach criterion

1) Flexibility 1: Number of trials to reverse a preference in the last reversal an individual experienced (reversal learning; as above)

2) Flexibility 3: If the number of trials to reverse a preference does not positively correlate with the latency to attempt or solve new loci on the multi-access box, then the **average latency to solve** and the **average latency to attempt** a new option on the multi-access box will be additional independent variables (as above).

4) Flexibility 4: This measure is currently being developed and is intended to be a more accurate representation of all the choices an individual made, as well as accounting for the degree of uncertainty exhibited by individuals as preferences change. If this measure more effectively represents flexibility (determined using a modeled dataset and not the actual data), we may decide to solely rely on this measure and not use flexibility measures 1 through 3. If this ends up being the case, we will modify the code in the analysis plan below to reflect this change.

Model 2b: latency to respond

1) Correct or incorrect response

2) Trial

3) [Flexibility Condition](./g_flexmanip.Rmd): control, flexibility manipulation

3) ID (random effect because multiple measures per bird)

NOTE Jul 2020: remove flexibility condition as a variable because, by definition, the birds in the manipulated group were faster to reverse their preferences.

##### *P1: detour*

1) Trial

NOTE (Aug 2020): Because the data are analyzed in a GLM, meaning that there is only one row per bird, trial number is not able to be included because it would need to be conducted on multiple rows per bird. Therefore, we removed this independent variable from this analysis.

2) Flexibility 1: Number of trials to reverse a preference in the last reversal an individual experienced (reversal learning; as above)

3) Flexibility 3: If the number of trials to reverse a preference does not positively correlate with the latency to attempt or solve new loci on the multi-access box, then the **average latency to solve** and the **average latency to attempt** a new option on the multi-access box will be additional independent variables (as above).

4) Flexibility 4: This measure is currently being developed and is intended to be a more accurate representation of all of the choices an individual made, as well as accounting for the degree of uncertainty exhibited by individuals as preferences change. If this measure more effectively represents flexibility (determined using a modeled dataset and not the actual data), we may decide to solely rely on this measure and not use flexibility measures 1 through 3. If this ends up being the case, we will modify the code in the analysis plan below to reflect this change.

##### *P3: does training improve detour performance?*

1) Condition: pre- or post-reversal learning tests


##### Unregistered analysis: Interobserver reliability of dependent variables

To determine whether experimenters coded the dependent variables in a repeatable way, hypothesis-blind video coders, Sophie Kaube (detour) and Brynna Hood (go/no go), were first trained in video coding the dependent variables (detour and go/no go: whether the bird made the correct choice or not), requiring a Cohen's unweighted kappa of 0.90 or above to pass training (using the psych package in R @psych). This threshold indicates that the two coders (the experimenter and the video coder) agree with each other to a high degree (@landis1977measurement). After passing training, the video coders coded 24% (detour) and 33% (go/no go) of the videos for each experiment and the unweighted Cohen's kappa was calculated to determine how objective and repeatable scoring was for this variable, while noting that the experimenter had the advantage over the video coder because watching the videos was not as clear as watching the bird participate in the trial from the aisle of the aviaries. The unweighted kappa was used because this is a categorical variable where the distances between the numbers are meaningless (0=incorrect choice, 1=correct choice, -1=did not participate).

**Detour: correct choice**

We randomly chose four (Diablo, Queso, Chalupa, and Habanero) of the 11 birds that had participated in this experiment by Nov 2019 using random.org. First, Kaube analyzed all videos from Habanero and Diablo, and we analyzed the data using an intraclass correlation coefficient, which is not an appropriate test for categorical data. After learning this, we switched to using the Cohen's unweighted kappa and replaced Habanero and Diablo with two new randomly chosen grackles (Mole and Chilaquile). Kaube then analyzed all videos from Queso and Chalupa for training and passed (Cohen's unweighted kappa=0.91, confidence boundary=0.75-1.00, n=24 data points). After passing training, Kaube analyzed all videos from Queso, Chalupa, Mole, and Chilaquile, and highly agreed with the experimenter's data (Cohen's unweighted kappa=0.91, confidence boundary=0.78-1.00, n=44 data points).

**Go/no go: correct choice**

We randomly chose three (Diablo, Burrito, and Chilaquile) of the 12 birds that were estimated to complete this experiment using random.org. Hood then analyzed all videos from Diablo for training and passed (Cohen's unweighted kappa=0.91, confidence boundary=0.80-1.00, n=40 data points). Hood then coded the rest of the videos and had substantial amounts of agreement with the experimenters (Cohen's unweighted kappa = 0.82, confidence boundary = 0.78-0.85, n=611 data points).

**Go/no go: latency to respond (peck the screen)**

Interobserver reliability was not conducted on this variable because we obtained this data from the automatically generated PsychoPy data sheets. However, we must note that when entering the latency to first screen peck into the main data sheet that the experimenter used to determine whether they made a correct choice or not, the two data sheets did not always match. This is because: 1) if a session started or ended with the bird not participating such that a trial was not triggered, this receives a -1 in the experimenter's data sheet and is not recorded by the PsychoPy data sheet; and 2) the touchscreen regularly failed to register screen pecks, which could result in an NA for the PsychoPy data sheet whereas the experimenter's data sheet recorded a choice.

```{r ior, eval=FALSE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}
library(irr) #ICC package

#### DETOUR

# did Sophie Kaube pass interobserver reliability training? YES (0.91) on 10 Feb 2020

## 9 Jan 2020 = ICC = 0.4 because we differed on 2/21 data points. However, Sophie was correct (CL checked the video) because she followed the instructions in an unbiased way. I need to rethink the instructions and then give her another video or two to code
ld <- c(1,1,-1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1) #live coder data from AdaptedFirstTouchCorrectChoice column for A036R- 2019-01-07 Detour S1 T1, A036R- 2019-01-08 Detour S2 T3, A064LR 2019-11-07 Detour S1 T1
vd <- c(1,1,-1,1,1,1,1,1,1,1,1,0,1,1,1,1,2,2,1,1,1) #video coder data for same videos
vdtest <- c(1,1,-1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1) #video coder data for same videos 
d <- data.frame(ld,vdtest)

icc(d, model = "oneway", type = "consistency", unit = "single", conf.level = 0.95) #=0.412 BUT WAIT!!!! The reason the ICC was so low is because it was looking at the distance between the measurements: 0-2 and 0-2 were the differences, but in reality, these are just categories that don't have a distance. So change the TYPE to CONSISTENCY rather than AGREEMENT (http://neoacademic.com/2011/11/16/computing-intraclass-correlations-icc-as-estimates-of-interrater-reliability-in-spss/). BUT the instructions for ICC say that if a oneway model is chosen, then consistency is the only option so it is implicitly already doing consistency.

## 10 Feb 2020: Did Sophie pass IOR for the second 2 birds she coded? YES 0.91
#NOTE: don't just add more videos to the above analysis to try to get her ICC up because it would take more than 4 extra videos to counteract the 2 disagreements. Instead, start from scratch on different videos and have her recode the above after she passes.
ld2 <- c(1,1,1,-1,1,2,-1,1,1,1,1,-1,2,1,1,1,0,1,1,1,1,1,-1,1) #live coder data from AdaptedFirstTouchCorrectChoice column for the same videos as vd2
vd2 <- c(0,1,1,-1,1,2,-1,1,1,1,1,-1,2,1,1,1,0,1,1,1,1,1,-1,1) #video coder data from AdaptedFirstTouchCorrectChoice for videos A025GO 2018-12-30 Detour Up S1 T1, A025GO 2018-12-31 Detour S2 T4, A025GO 2018-12-31 Detour S3 T6, A025GO 2018-12-31 Detour S4 T9, A025GO 2019-01-01 Detour S5 T10, A031-Y 2018-10-08 Detour S1 T1, A031-Y 2018-10-09 Detour S2 T10
d5 <- data.frame(ld2+2,vd2+2) 

cohen.kappa(d5, w=NULL,n.obs=NULL,alpha=.05,levels=NULL) #Sophie = unweighted kappa = 0.91, confidence boundary=0.75-1.00, n=24 data points
#Now have Sophie code 2 additional birds (don't need any more IOR because she already passed) and then 20% of the videos will be double coded

## 19 Feb 2020: Sophie IOR score for 20% of the videos (birds 25 Chalupa, 31 Queso, 35 Mole, 86 Chilaquile) = 0.91
ld3 <- c(1,1,1,-1,1,2,-1,1,1,1,1,-1,2,1,1,1,0,1,1,1,1,1,-1,1,  0,1,1,1,2,1,1,1,1,1,2,1,2,1,2,1,1,1,1,2) #live coder data from AdaptedFirstTouchCorrectChoice column for the same videos as vd2
vd3 <- c(0,1,1,-1,1,2,-1,1,1,1,1,-1,2,1,1,1,0,1,1,1,1,1,-1,1,  2,1,1,1,2,1,1,1,1,1,2,1,2,1,2,1,1,1,1,2) #video coder data from AdaptedFirstTouchCorrectChoice for videos A025GO 2018-12-30 Detour Up S1 T1, A025GO 2018-12-31 Detour S2 T4, A025GO 2018-12-31 Detour S3 T6, A025GO 2018-12-31 Detour S4 T9, A025GO 2019-01-01 Detour S5 T10, A031-Y 2018-10-08 Detour S1 T1, A031-Y 2018-10-09 Detour S2 T10, A035P- 2018-12-20 Detour S1 T1, A086GB 2020-01-01 Detour S1 T1
dfinal <- data.frame(ld3+2,vd3+2) 
cohen.kappa(dfinal, w=NULL,n.obs=NULL,alpha=.05,levels=NULL) #Sophie = unweighted kappa = 0.91, confidence boundary=0.78-1.00, n=44 data points


#### go/no go

# did Brynna Hood pass interobserver reliability training? YES Cohen's unweighted kappa = 0.91
mm <- c(-1,0,1,0,1,0,1,0,1,0,0,1,1,-1,1,0,1,0,1,0,1,0,1,0,0,1,0,1,1,0,0,1,1,0,0,0,0,1,1,0) #live coder data (Maggie MacPherson) from CorectChoice column for the same trials as in videos for bh
bh <- c(-1,2,1,0,1,0,1,0,1,0,0,1,1,-1,1,0,1,0,1,0,1,2,1,0,0,1,0,1,1,0,0,1,1,0,0,0,0,1,1,0) #video coder data from CorrectChoice for videos A064LR 2019-12-06 Go:No-Go S1 T1.mp4, A064LR 2019-12-06 Go:No-Go S2 T19.mp4
dgng <- data.frame(mm,bh) 
cohen.kappa(dgng, w=NULL,n.obs=NULL,alpha=.05,levels=NULL) #unweighted kappa = 0.91, confidence boundaries 0.80-1.00, n=40 data points

# interobserver reliability on 20% of the videos between Brynna Hood and the live coder (LC)
data <- read.csv("/Users/corina/ownCloud/Documents/Experiments/Interobserver Reliability/InterObsRelGoNoGoLCBrynna.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(data)  #Check to make sure it looks right
# Note: c(3,5) is telling R to look at columns 2 ('1CorrectChoice') and 3 ('2CorrectChoiceAdapted') and compare them. Double check this:
data[,3] #1CorrectChoice = live coder
data[,5] #2CorrectChoiceAdapted = re-aligned Coder2 data from Brynna
cohen.kappa(data[,c(3,5)], w=NULL,n.obs=NULL,alpha=.05,levels=NULL) #unweighted kappa = 0.82, confidence boundary = 0.78-0.85, n=611 data points
```



### E. ANALYSIS PLAN

We do not plan to **exclude** any data. When **missing data** occur, the existing data for that individual will be included in the analyses for the tests they completed. Analyses will be conducted in R (current version `r getRversion()`; @rcoreteam). When there is more than one experimenter within a test, experimenter will be added as a random effect to account for potential differences between experimenters in conducting the tests. If there are no differences between models including or excluding experimenter as a random effect, then we will use the model without this random effect for simplicity.

#### Ability to detect actual effects

To begin to understand what kinds of effect sizes we will be able to detect given our sample size limitations and our interest in decreasing noise by attempting to measure it, which increases the number of explanatory variables, we used G&ast;Power (v.3.1, @faul2007g, @faul2009statistical) to conduct power analyses based on confidence intervals. G&ast;Power uses pre-set drop down menus and we chose the options that were as close to our analysis methods as possible (listed in each analysis below). Note that there were no explicit options for GLMs (though the chosen test in G&ast;Power appears to align with GLMs) or GLMMs or for the inclusion of the number of trials per bird (which are generally large in our investigation), thus the power analyses are only an approximation of the kinds of effect sizes we can detect. We realize that these power analyses are not fully aligned with our study design and that these kinds of analyses are not appropriate for Bayesian statistics (e.g., our MCMCglmm below), however we are unaware of better options at this time. Additionally, it is difficult to run power analyses because it is unclear what kinds of effect sizes we should expect due to the lack of data on this species for these experiments. 

#### Data checking

The data will be visually checked to determine whether they are normally distributed via two methods: 1) normality is indicated when the histograms of actual data match those with simulated data (Figure 2), and 2) normality is indicated when the residuals closely fit the dotted line in the Normal Q-Q plot (Figure 3) [@zuur2009]. If the data do not appear normally distributed, visually check the residuals. If they are patternless, then assume a normal distribution [@zuur2009]. Detour data look normal, go/no go data are questionable, and both have patternless residuals, therefore we presume normality for both variables.

```{r dist_checkHist, eval=FALSE, warning=FALSE, results='asis', echo=TRUE}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","Gonogo150","Gonogo85","Detour","DetourA","Detourprepost","TrialsLast","AvgLatency")

#Check the dependent variables for normality: Histograms
op <- par(mfrow=c(2,4), mar=c(4,4,2,0.2))
#This is what the distribution of actual data looks like
hist(d$Gonogo150, xlab="Go/no go 150: Trials to criterion", main="Actual Data") #not normally distributed
hist(d$Gonogo85, xlab="Go/no go 85 (unregistered): Trials to criterion", main="Actual Data") #much more normally distributed than Gonogo150
hist(d$Detour, xlab="Detour: First approach", main="Actual Data")

#Given the actual data, this is what a normal distribution would look like
Y2 <- rnorm(1281, mean=mean(d$Gonogo150), sd=sd(d$Gonogo150))
hist(Y2, xlab="Go/no-go 150: Trials to criterion", main="Simulated Data") #The NAs break this, but looking at the histogram, the data are not normally distributed

Z2 <- rnorm(1281, mean=mean(d$Detour), sd=sd(d$Detour))
hist(Z2, xlab="Detour: First approach", main="Simulated Data")


#Check the dependent variables for normality: Q-Q plots
op <- par(mfrow=c(3,4), mar=c(4,4,2,0.2))
plot(glm(d$Gonogo150~d$TrialsLast)) 
plot(glm(d$Gonogo85~d$TrialsLast)) 
plot(glm(d$Detour~d$TrialsLast)) 
#detour looks normally distributed, go/no go 150 data are borderline, go/no go 85 data are normally distributed. Analyze all three dependent variables.

##Check the dependent variables for normality: Residuals
op <- par(mfrow=c(1,3), mar=c(4,4,2,0.2))
plot(residuals(glm(d$Detour~d$TrialsLast)), ylab="Detour residuals: First approach ~ Trials to reverse")
plot(residuals(glm(d$Gonogo150~d$TrialsLast)), ylab="Go-no-go 150: Residuals Correct response ~ Trials to reverse")
plot(residuals(glm(d$Gonogo85~d$TrialsLast)), ylab="Go-no-go 85 (unregistered): Residuals Correct response ~ Trials to reverse")
#All look patternless. Analyze all three dependent variables.

#The distribution of the actual data versus what a normal distribution would look like with simulated data. Residuals vs fitted: checking for homogeneity, which is satisfied if residuals have an even spread across the x-axis; Normal Q-Q: residuals are normally distributed if they are on the diagonal line; Residuals vs leverage: Cook's distance <1 means no influential observations [@zuur2009].
```


#### P1: delayed gratification

**Assess food preferences:** Conduct preference tests between pairs of different foods. Rank food preferences into three categories (High, Medium, Low) in the order of the percentage of times a food was chosen.

**Analysis:** Generalized Linear Model (GLM; glm function, stats package) with a Poisson distribution and log link, unless the only choices made were 0 (they didn't wait for food) and 1 (they waited for 1 piece of food but not for 2 or 3), in which case we will use a binomial distribution with a logit link. We will determine whether an independent variable had an effect or not using the Estimate in the full model.

To determine our ability to detect actual effects, we ran a power analysis in G&ast;Power with the following settings: test family=F tests, statistical test=linear multiple regression: Fixed model (R^2 deviation from zero), type of power analysis=a priori, alpha error probability=0.05. We reduced the power to 0.70 and increased the effect size until the total sample size in the output matched our projected sample size (n=32). The protocol of the power analysis is here:

*Input:*

Effect size fÂ²                	=	0,41
			
Î± err prob                    	=	0,05

Power (1-Î² err prob)          	=	0,7

Number of predictors          	=	5

*Output:*

Noncentrality parameter Î»     	=	13,1200000

Critical F                    	=	2,5867901

Numerator df                  	=	5

Denominator df                	=	26

Total sample size             	=	32

Actual power                  	=	0,7103096

This means that, with our sample size of 32, we have a 71% chance of detecting a large effect (approximated at f^2^=0.35 by @cohen1988statistical). 

```{r better, eval=FALSE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}
acc <- read.csv ("/Users/corina/GTGR/data/data_accumulation.csv", header=T, sep=",", stringsAsFactors=F) 

#GLM
better <- glm(NumberOfAccumulationsWaited ~ Delay + FoodQualityQuantity + Trial + TrialsToReverseLast, family="poisson", data=acc)
#summary(better)

better1 <- summary(better)
library(xtable)
better1.table <- xtable(better1)
library(knitr)
kable(better1.table, caption="Table U: Model selection output.", format="html", digits=2)
```

These analyses were not conducted because the experiment failed due to the grackles never habituating to the test apparatuses.

#### P1: go/no go

**Analysis:** 

**Model 2a: number of trials to reach criterion in the go/no go experiment** Generalized Linear Model (GLM; glm function, stats package) with a Poisson distribution and a log link. We will determine whether an independent variable had an effect or not using the Estimate in the full model.

To determine our ability to detect actual effects, we ran a power analysis in G&ast;Power with the following settings: test family=F tests, statistical test=linear multiple regression: Fixed model (R^2 deviation from zero), type of power analysis=a priori, alpha error probability=0.05. We reduced the power to 0.70 and increased the effect size until the total sample size in the output matched our projected sample size (n=32). The protocol of the power analysis is here:

*Input:*

Effect size fÂ²                	=	0,27

Î± err prob                    	=	0,05

Power (1-Î² err prob)          	=	0,7

Number of predictors          	=	2

*Output:*

Noncentrality parameter Î»     	=	8,6400000

Critical F                    	=	3,3276545

Numerator df                  	=	2

Denominator df                	=	29

Total sample size             	=	32

Actual power                  	=	0,7047420

This means that, with our sample size of 32, we have a 70% chance of detecting a medium (approximated at f^2^=0.15 by @cohen1988statistical) to large effect (approximated at f^2^=0.35 by @cohen1988statistical). 

```{r go, eval=F, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","Gonogo150","Gonogo85","Detour","DetourA","Detourprepost","TrialsFirst","TrialsLast","AvgLatency")

#GLM
m1 <- glm(Gonogo150 ~ TrialsLast, family="poisson", data=d)
#sm1 <- summary(m1) 
#there is no relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the last reversal)

m2 <- glm(Gonogo85 ~ TrialsLast, family="poisson", data=d)
#sm2 <- summary(m2) 
#there is a relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the last reversal) 

#have a look at the plot to see why m2, but not m1 is positively related
#plot(Gonogo85~TrialsLast, data=d) 
#Taquito is a huge outlier here and if I look at the rest of the data, I don't see a positive relationship. Need more data to know whether this relationship holds


#UNREGISTERED analyses: number of trials to reverse in the first reversal (which will make this data comparable with more species)
u1 <- glm(Gonogo150 ~ TrialsFirst, family="poisson", data=d)
#su1 <- summary(u1) 
#there is no relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the first reversal)

u2 <- glm(Gonogo85 ~ TrialsFirst, family="poisson", data=d)
#su2 <- summary(u2) 
#there is no relationship between the number of trials to pass criterion in go/no go and the number of trials to reverse a preference (in the first reversal) (though the estimate 0.008 and the standard error is lower than the estimate so it doesn't cross zero, it is is significant: p<2e-16)


#load packages for the output table
library(jtools) 
base::suppressMessages(jtools::export_summs(m1, m2, u1, u2, model.names=c("150 last reversal","85 last reversal","150 first reversal","85 first reversal"), digits = getOption("jtools-digits", default = 2), model.info = getOption("summ-model.info", TRUE), model.fit = getOption("summ-model.fit", TRUE), pvals = getOption("summ-pvals", FALSE))) 
#suppressMessages gets rid of the text saying that the broom package overwrites something in jtools. Need to specify the package before the function to avoid a message popping up and preventing the PDF from rendering
knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)}) 
#this prevents an issue with xcolor package when rendering to PDF


#Note: this code also works for showing output tables, but don't know how to get it to show more than one model at a time
#library(xtable)
#sm1.table <- xtable(sm1)
#library(knitr)
#kable(sm1.table, caption="Table T: Model selection output.", format="html", digits=2)
```


**Flexibility comprehensive:** In addition to the number of trials it took birds to reverse a preference, we also developed a more mechanistic measure of behavioral flexibility that takes into account all choices in the reversal learning experiment. Specifically, we use multilevel Bayesian reinforcement learning models that, from trial to trial, update the latent values of different options and use those *attractions* to explain observed choices. 

There are two basic components: 

First, we have an updating or learning equation that tells us how attractions to different behavioral options $A_{i,j, t+1}$ (i.e., how preferable option $i$ is to the bird $j$ at time $t+1$) change over time as a function of previous attractions $A_{i ,j, t}$ and recently experienced payoffs $\pi_{i,j,t}$ (i.e., whether they received a reward in a given trial or not). Attraction scores thus reflect the accumulated learning history up to this point. 

$A_{i,j, t+1} = (1-\phi_j) A_{i,j,t} + \phi_j \pi_{i,j,t}$. 

The (bird-specific) parameter $\phi_j$ describes the weight of recent experience. The higher the value of $\phi_j$, the faster the bird updates their attraction. It thus can be interpreted as the *learning or updating rate of an individual*. This corresponds to the first and third connotation of behavioral flexibility as defined by [@bond_serial_2007], the ability to rapidly and adaptively change behavior in light of new experiences. 

The second major part of the model expresses the probability an individual $j$ chooses option $i$ in the next round, $t+1$, based on the latent attractions:
 
 $P(i)_{t+1} = \frac{\exp(\lambda_j A_{i, j, t})}{\sum\limits_{m=1}^{2}\exp(\lambda_j A_{m, j, t})}.$
 
The parameter $\lambda_j$ represents the *random choice rate* of an individual (also called inverse temperature). It controls how sensitive choices are to differences in attraction scores. As $\lambda_j$ gets larger, choices become more deterministic, as it gets smaller, choices become more exploratory (random choice if $\lambda_j = 0$). This closely corresponds to the second connotation of internally generated behavioral variation, exploration or creativity [@bond_serial_2007]. To account for potential differences between experimenters, we also included experimenter ID as a random effect (omitted from previous equations to enhance readability, but available in the code below). 

This analysis yields posterior distributions for $\phi_j$ and $\lambda_j$ for each individual bird. To use these estimates in a GLM that predicts their causal score, we need to propagate the full *uncertainty* from the reinforcement learning model, which is achieved by directly passing the variables to the linear model within a single large *stan* model. We include both parameters ($\phi_j$ and $\lambda_j$) as predictors and estimate their respective independent effect on the number of trials to pass criterion in go/no go as well as an interaction term. To model the number of trials to pass criterion, we used a Poisson likelihood and a standard log link function as appropriate for count data with an unknown maximum. 

```{r reinforcement, eval=FALSE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}

# Load data
d_flex <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/data_reverseraw.csv"), header=T, sep=",", stringsAsFactors=F) 
d_inhibit <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F) 
d_detour <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_detour.csv"), header=F, sep=",", stringsAsFactors=F) 

library(rstan)

# Prepare Reversal learning data
d_flex <- subset(d_flex, d_flex$Reversal != "Control: Yellow Tube")
d_flex <- subset(d_flex, d_flex$CorrectChoice != -1)
d_flex$Reversal <- as.integer(d_flex$Reversal)
d_flex$Correct <- as.integer(d_flex$CorrectChoice)
d_flex$Trial <- as.integer(d_flex$Trial)
d_flex <- subset(d_flex, is.na(d_flex$Correct) == FALSE)

#Construct choice variable
d_flex$Choice <- NA
for (i in 1: nrow(d_flex)) {
  if (d_flex$Reversal[i] %in% seq(0, max(unique(d_flex$Reversal)), by = 2)){
    
    if (d_flex$Correct[i] == 1){
      d_flex$Choice[i] <- 1
    } else {
      d_flex$Choice[i] <- 2
    } 
  } else {
    if (d_flex$Correct[i] == 1){
      d_flex$Choice[i] <- 2
    } else {
      d_flex$Choice[i] <- 1
    } 
  }
}
d_flex <- d_flex[with(d_flex, order(d_flex$ID)), ]


# Go no go models
colnames(d_inhibit)[c(1,2,3)] <- c("ID", "GNG", "GNG85")
d_inhibit <- subset(d_inhibit, select = c(ID, GNG, GNG85))

# Keep only birds who finished the task
d_inhibit <- subset(d_inhibit, is.na(d_inhibit$GNG) == FALSE)

#Flexibility data for birds with inhibition score
d <- subset(d_flex, d_flex$ID %in% d_inhibit$ID)

# Sort birds alphabetically
d <- d[with(d, order(d$ID)), ]
d_inhibit   <- d_inhibit[with(d_inhibit, order(d_inhibit$ID)), ]

d$id <- sapply(1:nrow(d), function (i) which(unique(d$ID) == d$ID[i]) )
d$Expid <-  sapply(1:nrow(d), function (i) which(unique(d$Experimenter) == d$Experimenter[i]) )
d <- subset(d, select = c(id, Expid, Choice, Correct))

#Prepare data list for stan
dat <- as.list(d)
dat$N <- nrow(d)
dat$N_id <- length(unique(d$id))
dat$N_exp <- length(unique(d$Expid))
dat$z <- d_inhibit$GNG


reinforcement_poisson <- "

data{
   int N;
   int N_id;
   int N_exp;
   int id[N];
   int Expid[N];
   int Choice[N];
   int Correct[N];
   int z[N_id];
}

parameters{

  // Learning model
  real logit_phi;
  real log_L;

  // Varying effects clustered on individual
  matrix[2,N_id] z_ID;
  vector<lower=0>[2] sigma_ID;
  cholesky_factor_corr[2] Rho_ID;

  // Varying effects clustered on experimenter
  matrix[2,N_exp] z_EXP;
  vector<lower=0>[2] sigma_EXP;
  cholesky_factor_corr[2] Rho_EXP;

  // GLM
  real alpha;
  real b_phi;
  real b_lambda;
  real b_int;
}

transformed parameters{

matrix[N_id,2] v_ID; // varying effects on individuals
matrix[N_exp,2] v_EXP; // varying effects on experimenter

v_ID = ( diag_pre_multiply( sigma_ID , Rho_ID ) * z_ID )';
v_EXP = ( diag_pre_multiply( sigma_EXP , Rho_EXP ) * z_EXP )';
}

model{

matrix[N_id,2] A; // attraction matrix
vector[N_id] phi_i;
vector[N_id] lambda_i;

vector[N_id] phi_i_s ;
vector[N_id] lambda_i_s ;
vector[N_id] lambda;

alpha ~ normal(5,0.5);
b_phi ~ normal(0,0.3);
b_lambda ~ normal(0,0.3);
b_int ~ normal(0,0.3);

logit_phi ~  normal(0,2);
log_L ~  normal(0,2);

// varying effects
to_vector(z_ID) ~ normal(0,1);
sigma_ID ~ exponential(3);
Rho_ID ~ lkj_corr_cholesky(3);

to_vector(z_EXP) ~ normal(0,1);
sigma_EXP ~ exponential(3);
Rho_EXP ~ lkj_corr_cholesky(3);


// initialize attraction scores
for ( i in 1:N_id ) A[i,1:2] = rep_vector(0,2)';

// loop over Choices

for ( i in 1:N ) {
vector[2] pay;
vector[2] p;
real L;
real phi;

// first, what is log-prob of observed choice

L =  exp(log_L + v_ID[id[i],1] + v_EXP[Expid[i],1]);
p = softmax(L*A[id[i],1:2]' );
Choice[i] ~ categorical( p );

// second, update attractions conditional on observed choice

phi =  inv_logit(logit_phi +  v_ID[id[i],2] + v_EXP[Expid[i],2]);
pay[1:2] = rep_vector(0,2);
pay[ Choice[i] ] = Correct[i];
A[ id[i] , 1:2 ] = ( (1-phi)*to_vector(A[ id[i] , 1:2 ]) + phi*pay)';

}//i

// Define bird specific values on the outcome scale and standardize

lambda_i = exp(log_L + v_ID[,1]);
phi_i = inv_logit(logit_phi + v_ID[,2]);

lambda_i_s = (lambda_i - mean(lambda_i)) / sd(lambda_i);
phi_i_s = (phi_i - mean(phi_i)) / sd(phi_i);


lambda = alpha + b_lambda * lambda_i_s + b_phi * phi_i_s  + b_int * lambda_i_s .* phi_i_s;
z ~ poisson(exp(lambda));
}

"

m_gonogo <- stan( model_code  = reinforcement_poisson , data=dat ,iter = 5000, cores = 4, chains=4, control = list(adapt_delta=0.9999, max_treedepth = 13))  

# Set outcome to 85% criterion
dat$z <- d_inhibit$GNG85

m_gonogo85 <- stan( model_code  = reinforcement_poisson , data=dat ,iter = 5000, cores = 4, chains=4, control = list(adapt_delta=0.9999, max_treedepth = 13))  


#Detour models

d_detour <- subset(d_detour, d_detour$ID != "Pizza" & d_detour$FirstTouch != -1)
d_detour$AdaptedFirstTouch[which(d_detour$AdaptedFirstTouch == 2)] <- 1

#Keep learning data only from birds with inhibition score

d <- subset(d_flex, d_flex$ID %in% d_detour$ID)
d_detour <- subset(d_detour, d_detour$ID %in% d$ID)

# Sort birds alphabetically
d          <- d[with(d, order(d$ID)), ]
d_detour   <- d_detour[with(d_detour, order(d_detour$ID)), ]
d$id        <- sapply(1:nrow(d), function (i) which(unique(d$ID) == d$ID[i]) )
d_detour$id <- sapply(1:nrow(d_detour), function (i) which(unique(d_detour$ID) == d_detour$ID[i]) )
d$Expid <-  sapply(1:nrow(d), function (i) which(unique(d$Experimenter) == d$Experimenter[i]) )
dat <- as.list(subset(d, select = c(id, Expid, Choice, Correct)))
dat$N <- nrow(d)
dat$N_detour <- nrow(d_detour)
dat$N_id <- length(unique(d$id))
dat$N_exp <- length(unique(d$Expid))
dat$id_detour <- d_detour$id
dat$y <- d_detour$FirstTouch


reinforcement_binomial <- "

data{
   int N;
   int N_detour;
   int N_id;
   int N_exp;
   int id[N];
   int id_detour[N_detour];
   int Expid[N];
   int Choice[N];
   int Correct[N];
   int y[N_detour];
}

parameters{

  // Learning model
  real logit_phi;
  real log_L;

  // Varying effects clustered on individual
  matrix[2,N_id] z_ID;
  vector<lower=0>[2] sigma_ID;
  cholesky_factor_corr[2] Rho_ID;

  // Varying effects clustered on experimenter
  matrix[2,N_exp] z_EXP;
  vector<lower=0>[2] sigma_EXP;
  cholesky_factor_corr[2] Rho_EXP;

  // GLM
  real alpha;
  real b_phi;
  real b_lambda;
  real b_int;
}

transformed parameters{

matrix[N_id,2] v_ID; // varying effects on individuals
matrix[N_exp,2] v_EXP; // varying effects on experimenter

v_ID = ( diag_pre_multiply( sigma_ID , Rho_ID ) * z_ID )';
v_EXP = ( diag_pre_multiply( sigma_EXP , Rho_EXP ) * z_EXP )';
}

model{

matrix[N_id,2] A; // attraction matrix
vector[N_id] phi_i;
vector[N_id] lambda_i;

vector[N_id] phi_i_s ;
vector[N_id] lambda_i_s ;
vector[N_detour] prob;

alpha ~ normal(0,1.5);
b_phi ~ normal(0,0.2);
b_lambda ~ normal(0,0.2);
b_int ~ normal(0,0.2);

logit_phi ~  normal(0,2);
log_L ~  normal(0,2);

// varying effects
to_vector(z_ID) ~ normal(0,1);
sigma_ID ~ exponential(3);
Rho_ID ~ lkj_corr_cholesky(3);

to_vector(z_EXP) ~ normal(0,1);
sigma_EXP ~ exponential(3);
Rho_EXP ~ lkj_corr_cholesky(3);


// initialize attraction scores
for ( i in 1:N_id ) A[i,1:2] = rep_vector(0,2)';

// loop over Choices

for ( i in 1:N ) {
vector[2] pay;
vector[2] p;
real L;
real phi;

// first, what is log-prob of observed choice

L =  exp(log_L + v_ID[id[i],1] + v_EXP[Expid[i],1]);
p = softmax(L*A[id[i],1:2]' );
Choice[i] ~ categorical( p );

// second, update attractions conditional on observed choice

phi =  inv_logit(logit_phi +  v_ID[id[i],2] + v_EXP[Expid[i],2]);
pay[1:2] = rep_vector(0,2);
pay[ Choice[i] ] = Correct[i];
A[ id[i] , 1:2 ] = ( (1-phi)*to_vector(A[ id[i] , 1:2 ]) + phi*pay)';

}//i

// Define bird specific values on the outcome scale

for ( i in 1:N_id ) {
 lambda_i[i] = exp(log_L + v_ID[i,1]);
 phi_i[i] = inv_logit(logit_phi + v_ID[i,2]);
}

// Standardize predictors

lambda_i_s = (lambda_i - mean(lambda_i)) / sd(lambda_i);
phi_i_s = (phi_i - mean(phi_i)) / sd(phi_i);

// Binomial model
for ( i in 1:N_detour ) {
  prob[i] = alpha + b_lambda * lambda_i_s[id_detour[i]] + b_phi * phi_i_s[id_detour[i]]  + b_int * lambda_i_s[id_detour[i]] * phi_i_s[id_detour[i]];
  prob[i] = inv_logit(prob[i]);
}
y ~ binomial(1, prob);
}

"

m_detour <- stan( model_code  = reinforcement_binomial , data=dat ,iter = 5000, cores = 4, chains=4, control = list(adapt_delta=0.9999, max_treedepth = 10))  

# Grackle-specific scoring method
dat$y <- d_detour$AdaptedFirstTouch

m_detour_adapted <- stan( model_code  = reinforcement_binomial , data=dat ,iter = 5000, cores = 4, chains=4, control = list(adapt_delta=0.9999, max_treedepth = 10))  

### NOTE: To make the model work, you need to set up a few things... (this took Logan a few days because at every stage there is an error message and it isn't clear what the problem is or what to do next). It's best to go to https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started for instructions on how to install stan (and other dependencies) on your computer and get started. There are also plenty of stan fora where you can find answers to most common problems at https://discourse.mc-stan.org/. 

```



**Model 2b: latency to respond in the go/no go experiment** A Generalized Linear Mixed Model (GLMM; MCMCglmm function, MCMCglmm package; [@hadfieldMCMCglmmpackage]) will be used with a Poisson distribution and log link using 13,000 iterations with a thinning interval of 10, a burnin of 3,000, and minimal priors (V=1, nu=0) [@hadfield2014coursenotes]. We will ensure the GLMM shows acceptable convergence (lag time autocorrelation values <0.01 after lag 0; [@hadfieldMCMCglmmpackage]), and adjust parameters if necessary. We will determine whether an independent variable had an effect or not using the Estimate in the full model.

NOTE (Sep 2020): we changed the distribution to Gaussian (with an identity link) because MCMCglmm would not run on a Poisson (it kept saying there were negative integers even after we removed them). A Gaussian distribution also works for this kind of data because the response variable is a latency in seconds.

To roughly estimate our ability to detect actual effects (because these power analyses are designed for frequentist statistics, not Bayesian statistics), we ran a power analysis in G&ast;Power with the following settings: test family=F tests, statistical test=linear multiple regression: Fixed model (R^2 deviation from zero), type of power analysis=a priori, alpha error probability=0.05. We reduced the power to 0.70 and increased the effect size until the total sample size in the output matched our projected sample size (n=32). The number of predictor variables was restricted to only the fixed effects because this test was not designed for mixed models. The protocol of the power analysis is here:

*Input:*

Effect size fÂ²                	=	0,32

Î± err prob                    	=	0,05

Power (1-Î² err prob)          	=	0,7

Number of predictors          	=	3

*Output:*

Noncentrality parameter Î»     	=	10,2400000

Critical F                    	=	2,9466853

Numerator df                  	=	3

Denominator df                	=	28

Total sample size             	=	32

Actual power                  	=	0,7061592

This means that, with our sample size of 32, we have a 71% chance of detecting a large effect (approximated at f^2^=0.35 by @cohen1988statistical). 

```{r golatency, eval=FALSE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_data_gonogoExperimenter.csv"), header=T, sep=",", stringsAsFactors=F)
d <- data.frame(d)

#Remove trials that grackles didn't participate in, & Queso who didn't complete this experiment
d1 <- d[!d$CorrectChoice==-1  & !d$ID=="Queso",] 

#Note that latency data was brought in from PsychoPy data sheets, and that the number of trials reported by the experimenter and by PsychoPy sometimes differed for reasons that are not clear. Therefore, the first latency to peck the screen is not completely accurately matched between the two data sheets.

#Set numeric and factors so MCMCglmm will run
d1$LatencyFirstPeckToScreen <- as.numeric(d1$LatencyFirstPeckToScreen)
d1$Trial <- as.numeric(d1$Trial)
d1$CorrectChoice <- as.factor(d1$CorrectChoice)
d1$ID <- as.factor(d1$ID)

#GLMM
library(MCMCglmm)
prior2 = list(G=list(G1=list(V=1,nu=0)))

go <- MCMCglmm(LatencyFirstPeckToScreen ~ CorrectChoice * Trial, random=~ID, family="gaussian", data=d1, verbose=F, prior=prior2, nitt=3000000, thin=5000, burnin=2800000)
#gaussian = identity link for mcmcglmm https://cran.r-project.org/web/packages/MCMCglmm/vignettes/Overview.pdf
summary(go)
autocorr(go$Sol) 
#Did fixed effects converge? No, changed nitt and burning from 13000 and 3000 respectively but nothing is getting it to converge
autocorr(go$VCV) 
#Did random effects converge? No, changed nitt and burning from 13000 and 3000 respectively but nothing is getting it to converge
#The model did not converge so cannot include it in the results section
```

#### P1: detour

**Analysis:** Generalized Linear Model (GLM; glm function, stats package) with a binomial distribution and a logit link. We will determine whether an independent variable had an effect or not using the Estimate in the full model.

See the protocol for the power analyses for Model 2b above for the rough estimation of our ability to detect actual effects with this model.

```{r detour, eval=F, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","Gonogo150","Gonogo85","Detour","DetourA","Detourprepost","TrialsFirst","TrialsLast","AvgLatency")

#GLM
m4 <- glm(Detour ~ TrialsLast, family="binomial", data=d)
sm4 <- summary(m4) 
#there is no relationship between the proportion of trials correct on the detour task and the number of trials to reverse in the last reversal

#UNREGISTERED ANALYSIS: first reversal (rather than last reversal)
m5 <- glm(Detour ~ TrialsFirst, family="binomial", data=d)
sm5 <- summary(m5) 

#load packages for the output table
library(jtools) 
base::suppressMessages(jtools::export_summs(m4, m5, model.names=c("last reversal","first reversal"), digits = getOption("jtools-digits", default = 2), model.info = getOption("summ-model.info", TRUE), model.fit = getOption("summ-model.fit", TRUE), pvals = getOption("summ-pvals", FALSE))) 
#suppressMessages gets rid of the text saying that the broom package overwrites something in jtools. Need to specify the package before the function to avoid a message popping up and preventing the PDF from rendering
knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)}) 
#this prevents an issue with xcolor package when rendering to PDF
```

**Flexibility comprehensive:** We again repeat the analyses for the detour task with the more comprehensive computational measure of behavioral flexibility that takes into account all choices in the reversal learning experiment. We include both parameters ($\phi_j$ and $\lambda_j$) as well as their interaction to predict whether birds make correct choices in each trial of the detour task. We use a binomial likelihood as the outcome distribution and a logit link function (see section 2a for full data preparation and analysis script).




#### P1 alternative 2: are behavioral inhibition results reliable?

The reliability of the behavioral inhibition tests will be calculated using Cronbach's Alpha (as in @friedman2004relations; R package: psy [@psy], function: cronbach), which is indicated by alpha in the output.

NOTE (Sep 2020): when we tried to run this code we discovered that this is not the appropriate test to run on our experimental design to test the internal validity of the experiment (e.g., does this test actually measure what we think it does). To test internal validity, we would need to change the experimental design, which was not the goal of our current study. Therefore, we did not conduct this analysis.

```{r reliability, eval=FALSE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}
dat <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datareliabilityanalysis.csv"), header=T, sep=",", stringsAsFactors=F)
dat <- data.frame(dat)
#Note: n=8 grackles participated in both of these tests (1 is female). Had to make IDs numeric for the code below to run

#Load package
library(psych)

#Compare go/no go 150 with detour standard
a1 <- data.frame(dat$gng150)
alpha(a1) #

#Compare go/no go 85 with detour standard
a2 <- data.frame(dat$gng85,dat$detour)
cronbach(da2) #alpha

#Compare go/no go 150 with detour grackle specific
a3 <- data.frame(dat$gng150,dat$detoura)
cronbach(da3) #alpha

#Compare go/no go 85 with detour grackle specific
a4 <- data.frame(dat$gng85,dat$detoura)
cronbach(da4) #alpha
```


#### P2: correlation across behavioral inhibition tasks

**See analysis description for P1 alternative 2.** 

```{r reliability2, eval=FALSE, warning=FALSE, results='asis', echo=TRUE, include=TRUE}
dat <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datareliabilityanalysis.csv"), header=T, sep=",", stringsAsFactors=F)
dat <- data.frame(dat)
#Note: n=8 grackles participated in both of these tests (1 is female). Had to make IDs numeric for the code below to run
#Details on alpha at https://data.library.virginia.edu/using-and-interpreting-cronbachs-alpha/. 0=variables are independent and don't measure the same trait, 1=the two measures measure the same trait

#Load package
library(psy)

#Compare go/no go 150 with detour standard
da <- data.frame(dat$gng150,dat$detour)
cronbach(da) #alpha 0.02756873, the two variables are independent of each other

#Compare go/no go 85 with detour standard
da2 <- data.frame(dat$gng85,dat$detour)
cronbach(da2) #alpha 0.004805139, the two variables are independent of each other

#Compare go/no go 150 with detour grackle specific
da3 <- data.frame(dat$gng150,dat$detoura)
cronbach(da3) #alpha 0.03041709, the two variables are independent of each other

#Compare go/no go 85 with detour grackle specific
da4 <- data.frame(dat$gng85,dat$detoura)
cronbach(da4) #alpha 0.002771009, the two variables are independent of each other
```

#### P3: does training improve detour performance?

**Analysis:** Generalized Linear Model (GLM; glm function, stats package) with a binomial distribution and a logit link. We will determine whether an independent variable had an effect or not using the Estimate in the full model.

To determine our ability to detect actual effects, we ran a power analysis in G&ast;Power with the following settings: test family=F tests, statistical test=linear multiple regression: Fixed model (R^2 deviation from zero), type of power analysis=a priori, alpha error probability=0.05. We reduced the power to 0.70 and increased the effect size until the total sample size in the output matched our projected sample size (n=32). The protocol of the power analysis is here:

*Input:*

Effect size fÂ²                	=	0,21

Î± err prob                    	=	0,05

Power (1-Î² err prob)          	=	0,7

Number of predictors          	=	1

*Output:*

Noncentrality parameter Î»     	=	6,7200000

Critical F                    	=	4,1708768

Numerator df                  	=	1

Denominator df                	=	30

Total sample size             	=	32

Actual power                  	=	0,7083763

This means that, with our sample size of 32, we have a 71% chance of detecting a medium effect (approximated at f^2^=0.15 by @cohen1988statistical).

```{r detour2, eval=F, warning=F, results='asis', echo=T, include=T}
d <- read.csv(url("https://raw.githubusercontent.com/corinalogan/grackles/master/Files/Preregistrations/g_inhibition_datasummary.csv"), header=F, sep=",", stringsAsFactors=F)
d <- data.frame(d)
colnames(d) <- c("Bird","ID","Gonogo150","Gonogo85","Detour","DetourA","Detourprepost","TrialsFirst","TrialsLast","AvgLatency")

#GLM - detour standard correct choice scoring
de <- glm(Detour ~ Detourprepost, family="binomial", data=d)
sde <- summary(de)
#no difference between detour percent correct and whether they received detour before or after reversal tubes

#GLM - detour grackle-specific correct choice scoring - UNREGISTERED ANALYSIS
de2 <- glm(DetourA ~ Detourprepost, family="binomial", data=d)
sde2 <- summary(de2)
#no difference between detour percent correct and whether they received detour before or after reversal tubes

#load packages for the output table
library(jtools) 
base::suppressMessages(jtools::export_summs(de, de2, model.names=c("Detour standard","Detour grackle-specific"), digits = getOption("jtools-digits", default = 2), model.info = getOption("summ-model.info", TRUE), model.fit = getOption("summ-model.fit", TRUE), pvals = getOption("summ-pvals", FALSE))) 
#suppressMessages gets rid of the text saying that the broom package overwrites something in jtools. Need to specify the package before the function to avoid a message popping up and preventing the PDF from rendering
knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)}) 
#this prevents an issue with xcolor package when rendering to PDF
```

#### Alternative Analyses

We anticipate that we will want to run additional/different analyses after reading @statrethinkingbook. We will revise this preregistration to include these new analyses before conducting the analyses above. See the [State of the Data](#a-state-of-the-data) for a description of the analysis changes we made.

### F. PLANNED SAMPLE

Great-tailed grackles are caught in the wild in Tempe, Arizona, USA, for individual identification (colored leg bands in unique combinations). Some individuals (~32) are brought temporarily into aviaries for testing, and then they will be released back to the wild. Grackles are individually housed in an aviary (each 244cm long by 122cm wide by 213cm tall) at Arizona State University for a maximum of three months where they have ad lib access to water at all times and are fed Mazuri Small Bird maintenance diet ad lib during non-testing hours (minimum 20h per day), and various other food items (e.g., peanuts, grapes, bread) during testing (up to 3h per day per bird). Individuals are given three to four days to habituate to the aviaries and then their test battery begins on the fourth or fifth day (birds are usually tested six days per week, therefore if their fourth day in the aviaries occurs on a day off, then they are tested on the fifth day instead).

**Sample size rationale**

We will test as many birds as we can in the approximately three years at this field site given that the birds only participate in tests in aviaries during the non-breeding season (approximately September through March). The minimum sample size will be 16, however we expect to be able to test up to 32 grackles.

**Data collection stopping rule**

We will stop testing birds once we have completed two full aviary seasons (likely in March 2020). NOTE: the two full aviary seasons concluded in May 2020.

### G. ETHICS

This research is carried out in accordance with permits from the:

1) US Fish and Wildlife Service (scientific collecting permit number MB76700A-0,1,2)
2) US Geological Survey Bird Banding Laboratory (federal bird banding permit number 23872)
3) Arizona Game and Fish Department (scientific collecting license number SP594338 [2017], SP606267 [2018], and SP639866 [2019])
4) Institutional Animal Care and Use Committee at Arizona State University (protocol number 17-1594R)
5) University of Cambridge ethical review process (non-regulated use of animals in scientific procedures: zoo4/17)

### H. AUTHOR CONTRIBUTIONS

**Logan:** Hypothesis development, experimental design (go/no go task), data collection, data analysis and interpretation, write up, revising/editing, materials/funding.

**McCune:** Data collection, data interpretation, revising/editing.

**MacPherson:** Data collection, data interpretation, revising/editing.

**Johnson-Ulrich:** Touchscreen programming for go/no go task, data interpretation, revising/editing.

**Bergeron:** Data collection, data interpretation, revising/editing.

**Rowney:** Data collection, data interpretation, revising/editing.

**Seitz:** Experimental design (go/no go task), touchscreen programming (go/no go task), data interpretation, revising/editing.

**Blaisdell:** Experimental design (go/no go task), data interpretation, revising/editing.

**Folsom:** Data collection, data interpretation, revising/editing.

**Deffner:** Data analysis (Flexibility 4 model), revising/editing.

**Wascher:** Hypothesis development, experimental design (delayed gratification and detour tasks), data analysis and interpretation, write up, revising/editing.

### I. FUNDING

This research is funded by the Department of Human Behavior, Ecology and Culture at the Max Planck Institute for Evolutionary Anthropology, and by a Leverhulme Early Career Research Fellowship to Logan in 2017-2018.

### J. CONFLICT OF INTEREST DISCLOSURE

We, the authors, declare that we have no financial conflicts of interest with the content of this article. Corina Logan is a Recommender and on the Managing Board at PCI Ecology.

### K. ACKNOWLEDGEMENTS

We thank Dieter Lukas for help polishing the predictions; Ben Trumble for providing us with a wet lab at Arizona State University and Angela Bond for lab support; Melissa Wilson for sponsoring our affiliations at Arizona State University and lending lab equipment; Kevin Langergraber for serving as local PI on the ASU IACUC; Kristine Johnson for technical advice on great-tailed grackles; Arizona State University School of Life Sciences Department Animal Care and Technologies for providing space for our aviaries and for their excellent support of our daily activities; Julia Cissewski for tirelessly solving problems involving financial transactions and contracts; Richard McElreath for project support; Erin Vogel, our Recommender at PCI Ecology, and Simon Gingins and two anonymous reviewers for their wonderful feedback; Debbie Kelly for advice on how to modify the go/no go experiment; and our research assistants: Aelin Mayer, Nancy Rodriguez, Brianna Thomas, Aldora Messinger, Elysia Mamola, Michael Guillen, Rita Barakat, Adriana Boderash, Olateju Ojekunle, August Sevchik, Justin Huynh, Jennifer Berens, and Amanda Overholt.

### L. REFERENCES
